# Optimization-Methods
Este reposit√≥rio cont√©m implementa√ß√µes de m√©todos de otimiza√ß√£o num√©rica, incluindo:

- Gradiente Descendente
- M√©todo de Newton
- M√©todo BFGS
- Busca linear (Armijo)
- C√°lculo de gradientes e Hessianas via diferen√ßas finitas

As implementa√ß√µes s√£o feitas em Python utilizando NumPy e Matplotlib para visualiza√ß√£o.

### üìå Funcionalidades

- Suporte para gradiente anal√≠tico e aproxima√ß√£o por diferen√ßas finitas.
- Op√ß√£o de busca linear para escolha adaptativa do tamanho do passo.
- Visualiza√ß√£o da trajet√≥ria de otimiza√ß√£o em fun√ß√µes bidimensionais.
- Implementa√ß√£o modular, permitindo f√°cil extens√£o para novos m√©todos.

### üöÄ Aplica√ß√µes em Machine Learning
A otimiza√ß√£o num√©rica desempenha um papel fundamental no treinamento de modelos de Machine Learning. Os m√©todos implementados neste reposit√≥rio podem ser usados para:

- Treinamento de Redes Neurais: O Gradiente Descendente √© a base para algoritmos como SGD, Adam e RMSprop, usados no ajuste dos pesos de redes neurais profundas.
- Regress√£o Log√≠stica e Linear: O m√©todo de Newton pode acelerar a converg√™ncia nesses modelos convexos.
- SVMs (M√°quinas de Vetores de Suporte): Algoritmos de otimiza√ß√£o como BFGS podem ser utilizados para minimizar fun√ß√µes de perda convexas.
- Ajuste de Hiperpar√¢metros: M√©todos de otimiza√ß√£o podem ser empregados para encontrar hiperpar√¢metros √≥timos em modelos de ML.

### üí° Como Usar
Exemplo: Otimiza√ß√£o da Fun√ß√£o de Rosenbrock
```python
import numpy as np
from optimization_methods import gd, f2, grad_f2  

x0 = np.array([0, 0])  # Ponto inicial  
x_opt, iters = gd(f2, x0, grad_f2, plot=True)  

print(f"Resultado: {x_opt}")  
print(f"Itera√ß√µes: {iters}")
```
Exemplo: Aplica√ß√£o na Minimiza√ß√£o de Erro em Regress√£o
```python
import numpy as np
from optimization_methods import gd, fin_diff  

# Fun√ß√£o de erro quadr√°tico para regress√£o linear  
def mse_loss(w):  
    X = np.array([[1, 1], [1, 2], [1, 3]])  # Dados  
    y = np.array([2, 2.5, 3.5])  # R√≥tulos  
    pred = X @ w  
    return np.mean((pred - y) ** 2)  

x0 = np.array([0.0, 0.0])  # Inicializa√ß√£o dos pesos  
grad = lambda x: fin_diff(mse_loss, x, degree=1)  # Gradiente via diferen√ßas finitas  

x_opt, iters = gd(mse_loss, x0, grad, plot=False)  
print(f"Pesos √≥timos: {x_opt}")
```
